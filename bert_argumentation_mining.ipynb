{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Argumentation Mining with BERT\n",
        "# By Zaher Alkaei\n",
        "\n",
        "This notebook fine-tunes a BERT model to classify argumentative components in US Presidential Campaign Debate transcripts.\n",
        "\n",
        "# Dataset Source:\n",
        "Yes, we can! Mining Arguments in 50 Years of US Presidential Campaign Debates (Haddadan et al., ACL 2019)\n",
        " https://github.com/ElecDeb60To16/Dataset\n",
        "\n",
        "# Labels:\n",
        "- O: Non-argumentative\n",
        "-Claim\n",
        "-Premise\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#What does training BERT mean here?\n",
        "We use a pretrained BERT model (bert-base-uncased) and fine-tune it for our classification task.\n",
        "This involves keeping BERT's original layers, replacing the final classification layer with a new one for 3 classes,\n",
        "and training the model on our dataset to adjust weights for better predictions."
      ],
      "metadata": {
        "id": "Nx7e5rpbkZ2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install Required Libraries"
      ],
      "metadata": {
        "id": "JK5xox2yk0WC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM7uZOTTk-qG",
        "outputId": "7d0d6ebd-709d-45e5-c908-b74d92244530"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Load and Explore Dataset"
      ],
      "metadata": {
        "id": "GaHdGiRGk5VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"sentence_db_candidate.csv\")\n",
        "\n",
        "# Drop rows with missing Speech or Component\n",
        "df = df.dropna(subset=[\"Speech\", \"Component\"])\n",
        "\n",
        "# Show basic statistics\n",
        "print(\"Total samples:\", len(df))\n",
        "print(\"\\nClass distribution:\")\n",
        "print(df[\"Component\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPqFXSQok9q6",
        "outputId": "cca920b4-5d6d-4938-efd4-d478390d3a67"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 29532\n",
            "\n",
            "Class distribution:\n",
            "Component\n",
            "Claim      11964\n",
            "Premise    10316\n",
            "O           7252\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Encode Labels and Split Data"
      ],
      "metadata": {
        "id": "5_LgRInqlEvp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TrhZgOyWgK4m"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Map text labels to numeric labels\n",
        "label_map = {\"O\": 0, \"Claim\": 1, \"Premise\": 2}\n",
        "df[\"label\"] = df[\"Component\"].map(label_map)\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df[\"label\"], random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Tokenize Sentences and Prepare Datasets"
      ],
      "metadata": {
        "id": "7gUC7zeAlLsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(df[\"label\"]),\n",
        "    y=df[\"label\"]\n",
        ")\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "class ArgumentDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.encodings = tokenizer(list(dataframe[\"Speech\"]), truncation=True, padding=True, return_tensors=\"pt\")\n",
        "        self.labels = list(dataframe[\"label\"])\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset = ArgumentDataset(train_df)\n",
        "val_dataset = ArgumentDataset(val_df)\n",
        "test_dataset = ArgumentDataset(test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLEpWP9qlMed",
        "outputId": "2f7ac11e-2913-46e5-8305-34e1f0f423b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights: tensor([1.3574, 0.8228, 0.9542])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Load Pretrained BERT Model"
      ],
      "metadata": {
        "id": "44rXQmykmm6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "# Load BERT with classification head for 3 classes\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBN_skOJmj_k",
        "outputId": "355a2c93-90c9-447f-c309-26614d357586"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Define Training Arguments"
      ],
      "metadata": {
        "id": "Ia4Rt0mTlUzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, EarlyStoppingCallback, Trainer\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, *args, class_weights=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights.to(self.model.device)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = CrossEntropyLoss(weight=self.class_weights)\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.03,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eedTnfV3lVYj",
        "outputId": "981a49c3-ad8b-4625-f1a0-0ee3524f117f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Train the Model"
      ],
      "metadata": {
        "id": "9bHAbkeslaKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    class_weights=class_weights,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "S7o1NEuLlak5",
        "outputId": "6c3f377f-3224-4272-cde4-a5a252dc4818"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2584' max='2584' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2584/2584 05:25, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.841400</td>\n",
              "      <td>0.792331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.755200</td>\n",
              "      <td>0.797313</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2584, training_loss=0.8162254504744113, metrics={'train_runtime': 326.3834, 'train_samples_per_second': 126.673, 'train_steps_per_second': 7.917, 'total_flos': 3973078386047232.0, 'train_loss': 0.8162254504744113, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Evaluate the Model"
      ],
      "metadata": {
        "id": "ce7tV48tlheb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Predict on test set\n",
        "predictions = trainer.predict(test_dataset)\n",
        "y_true = [example[\"labels\"].item() for example in test_dataset]\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=list(label_map.keys()), digits=2))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "iwTnc6UTlh4l",
        "outputId": "df8a7a5d-33eb-49ef-8680-f4ae5f0d6237"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           O       0.67      0.64      0.65      1088\n",
            "       Claim       0.65      0.71      0.68      1795\n",
            "     Premise       0.63      0.58      0.60      1547\n",
            "\n",
            "    accuracy                           0.65      4430\n",
            "   macro avg       0.65      0.64      0.64      4430\n",
            "weighted avg       0.65      0.65      0.64      4430\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Save and Download the Model (For Colab)\n",
        "This step allows you to download the model easily from Google Colab."
      ],
      "metadata": {
        "id": "-xk7iLkLlqrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./bert_argument_model\")\n",
        "tokenizer.save_pretrained(\"./bert_argument_model\")\n",
        "\n",
        "import shutil\n",
        "from IPython.display import FileLink\n",
        "\n",
        "shutil.make_archive(\"bert_argument_model\", 'zip', \"./bert_argument_model\")\n",
        "FileLink(\"bert_argument_model.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "TBrRBX7llrAt",
        "outputId": "8f974010-1fa2-4a08-906e-9525ed3b5bc7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/bert_argument_model.zip"
            ],
            "text/html": [
              "<a href='bert_argument_model.zip' target='_blank'>bert_argument_model.zip</a><br>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10: Explain the Model\n",
        "This step helps us understand which words influence the model's decisions.\n",
        "We use Integrated Gradients to find which tokens are most important for each predicted class."
      ],
      "metadata": {
        "id": "7nQsss4gp9u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install captum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV6XW6SipvNX",
        "outputId": "a7fa4134-6d46-4160-90b1-9ec3af1daddd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: captum in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from captum) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (from captum) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from captum) (24.2)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.11/dist-packages (from captum) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from captum) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10->captum) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10->captum) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def get_token_attributions(text, target_class):\n",
        "    # Tokenize\n",
        "    encoding = tokenizer(text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=64)\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Get input embeddings\n",
        "    inputs_embeds = model.get_input_embeddings()(input_ids)\n",
        "\n",
        "    def forward_func(inputs_embeds):\n",
        "        outputs = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
        "        return outputs.logits\n",
        "\n",
        "    ig = IntegratedGradients(forward_func)\n",
        "    attributions, _ = ig.attribute(\n",
        "        inputs=inputs_embeds,\n",
        "        target=target_class,\n",
        "        return_convergence_delta=True\n",
        "    )\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "    scores = attributions.sum(dim=-1).squeeze(0)\n",
        "\n",
        "    return [(tok, float(score)) for tok, score in zip(tokens, scores)]\n",
        "\n",
        "# Accumulate scores by class\n",
        "token_scores_by_class = {0: Counter(), 1: Counter(), 2: Counter()}\n",
        "added_counts = {0: 0, 1: 0, 2: 0}\n",
        "\n",
        "for i in tqdm(range(len(test_df)), desc=\"Processing test examples\"):\n",
        "    text = test_df.iloc[i][\"Speech\"]\n",
        "\n",
        "    encoding = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    encoding = {key: val.to(device) for key, val in encoding.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoding)\n",
        "\n",
        "    pred_class = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "    try:\n",
        "        token_scores = get_token_attributions(text, pred_class)\n",
        "    except Exception as e:\n",
        "        print(f\"Error on sample {i}: {e}\")\n",
        "        continue\n",
        "\n",
        "    added = 0  # how many tokens are actually added\n",
        "    for token, score in token_scores:\n",
        "        token_clean = token.strip(\"Ġ▁#\")\n",
        "        if token_clean not in tokenizer.all_special_tokens and token_clean.isalpha():\n",
        "            token_scores_by_class[pred_class][token_clean.lower()] += abs(score)\n",
        "            added += 1\n",
        "\n",
        "    added_counts[pred_class] += added\n",
        "\n",
        "    # Print debug info every 100 examples\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Processed example {i}: class {pred_class}, {added} tokens added\")\n",
        "\n",
        "# Show counts of added tokens\n",
        "print(\"\\n[Debug] Number of tokens added per class:\")\n",
        "for cls, count in added_counts.items():\n",
        "    print(f\"Class {cls} ({label_names[cls]}): {count} tokens\")\n",
        "\n",
        "# Display top tokens\n",
        "label_names = {0: \"O (Non-argumentative)\", 1: \"Claim\", 2: \"Premise\"}\n",
        "\n",
        "for cls in [0, 1, 2]:\n",
        "    print(f\"\\nTop 10 words for class: {label_names[cls]}\")\n",
        "    for word, score in token_scores_by_class[cls].most_common(10):\n",
        "        print(f\"{word}: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ygd5HTU27K9k",
        "outputId": "d7490e31-188d-4420-f4ad-babac6ab1935"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:   0%|          | 2/4430 [00:00<08:57,  8.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 0: class 1, 24 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:   2%|▏         | 102/4430 [00:11<08:12,  8.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 100: class 2, 28 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:   5%|▍         | 202/4430 [00:22<07:55,  8.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 200: class 1, 31 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:   7%|▋         | 302/4430 [00:33<07:45,  8.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 300: class 2, 17 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:   9%|▉         | 402/4430 [00:45<07:36,  8.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 400: class 2, 26 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  11%|█▏        | 502/4430 [00:56<07:19,  8.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 500: class 1, 11 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  14%|█▎        | 602/4430 [01:07<07:12,  8.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 600: class 1, 9 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  16%|█▌        | 702/4430 [01:18<06:58,  8.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 700: class 1, 9 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  18%|█▊        | 802/4430 [01:30<06:48,  8.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 800: class 2, 25 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  20%|██        | 902/4430 [01:41<06:36,  8.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 900: class 1, 23 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  23%|██▎       | 1002/4430 [01:52<06:27,  8.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 1000: class 1, 46 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  25%|██▍       | 1102/4430 [02:03<06:11,  8.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 1100: class 0, 14 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  27%|██▋       | 1202/4430 [02:15<06:03,  8.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 1200: class 0, 7 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  29%|██▉       | 1302/4430 [02:26<05:52,  8.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 1300: class 2, 7 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  32%|███▏      | 1402/4430 [02:37<05:43,  8.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 1400: class 0, 21 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  34%|███▍      | 1502/4430 [02:48<05:29,  8.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 1500: class 1, 7 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  36%|███▌      | 1602/4430 [03:00<05:20,  8.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 1600: class 2, 22 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  38%|███▊      | 1702/4430 [03:11<05:05,  8.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 1700: class 1, 6 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  41%|████      | 1802/4430 [03:22<04:55,  8.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 1800: class 1, 22 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  43%|████▎     | 1902/4430 [03:34<04:43,  8.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 1900: class 0, 11 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  45%|████▌     | 2002/4430 [03:45<04:34,  8.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 2000: class 2, 43 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  47%|████▋     | 2102/4430 [03:56<04:20,  8.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 2100: class 0, 2 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  50%|████▉     | 2202/4430 [04:07<04:10,  8.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 2200: class 2, 9 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  52%|█████▏    | 2302/4430 [04:19<03:59,  8.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 2300: class 0, 9 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  54%|█████▍    | 2402/4430 [04:30<03:48,  8.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 2400: class 0, 18 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  56%|█████▋    | 2502/4430 [04:41<03:35,  8.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 2500: class 1, 20 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  59%|█████▊    | 2602/4430 [04:52<03:26,  8.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 2600: class 1, 24 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  61%|██████    | 2702/4430 [05:04<03:12,  8.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 2700: class 1, 6 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  63%|██████▎   | 2802/4430 [05:15<03:01,  8.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 2800: class 1, 23 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  66%|██████▌   | 2902/4430 [05:26<02:50,  8.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 2900: class 0, 11 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  68%|██████▊   | 3002/4430 [05:37<02:41,  8.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 3000: class 1, 11 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  70%|███████   | 3102/4430 [05:48<02:28,  8.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 3100: class 1, 25 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  72%|███████▏  | 3202/4430 [06:00<02:17,  8.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 3200: class 1, 13 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  75%|███████▍  | 3302/4430 [06:11<02:06,  8.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 3300: class 2, 13 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  77%|███████▋  | 3402/4430 [06:22<01:55,  8.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 3400: class 1, 27 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  79%|███████▉  | 3502/4430 [06:34<01:45,  8.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 3500: class 1, 26 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  81%|████████▏ | 3602/4430 [06:45<01:32,  8.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 3600: class 1, 24 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  84%|████████▎ | 3702/4430 [06:56<01:21,  8.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 3700: class 2, 26 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  86%|████████▌ | 3802/4430 [07:07<01:10,  8.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 3800: class 2, 14 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  88%|████████▊ | 3902/4430 [07:19<00:59,  8.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 3900: class 0, 8 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  90%|█████████ | 4002/4430 [07:30<00:48,  8.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 4000: class 2, 6 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  93%|█████████▎| 4102/4430 [07:41<00:37,  8.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 4100: class 1, 31 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  95%|█████████▍| 4202/4430 [07:52<00:25,  8.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 4200: class 2, 14 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  97%|█████████▋| 4302/4430 [08:04<00:14,  8.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 4300: class 2, 23 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples:  99%|█████████▉| 4402/4430 [08:15<00:03,  8.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed example 4400: class 1, 34 tokens added\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test examples: 100%|██████████| 4430/4430 [08:18<00:00,  8.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Debug] Number of tokens added per class:\n",
            "Class 0 (O (Non-argumentative)): 11023 tokens\n",
            "Class 1 (Claim): 35326 tokens\n",
            "Class 2 (Premise): 28778 tokens\n",
            "\n",
            "Top 10 words for class: O (Non-argumentative)\n",
            "i: 52.4445\n",
            "the: 37.6329\n",
            "you: 31.5751\n",
            "is: 30.6473\n",
            "s: 28.1265\n",
            "question: 22.3725\n",
            "and: 22.1728\n",
            "that: 21.9825\n",
            "in: 21.9062\n",
            "to: 21.5119\n",
            "\n",
            "Top 10 words for class: Claim\n",
            "i: 108.5829\n",
            "to: 100.7928\n",
            "we: 84.5858\n",
            "think: 83.8396\n",
            "the: 74.4226\n",
            "is: 72.8829\n",
            "not: 58.0119\n",
            "that: 51.3388\n",
            "a: 49.0987\n",
            "and: 47.4262\n",
            "\n",
            "Top 10 words for class: Premise\n",
            "the: 46.5082\n",
            "in: 43.1278\n",
            "i: 34.6165\n",
            "they: 31.4804\n",
            "said: 28.1193\n",
            "and: 27.8060\n",
            "to: 20.2946\n",
            "a: 18.9482\n",
            "of: 18.9305\n",
            "we: 17.8354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}